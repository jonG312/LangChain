{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPL50tZjT70+n2Dl5vfhVb5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonG312/LangChain-/blob/main/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic congfig\n",
        "# Install basic package and set key\n",
        "!pip install python-dotenv==1.0.0\n",
        "!pip install langchain==0.0.137\n",
        "!pip install pinecone-client==2.2.1\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "Ex5tWwMg_NlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = ''"
      ],
      "metadata": {
        "id": "NntLLtHO9c9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run basic query with OpenAI wrapper\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "text = \"What would be a good company name for a company that make colorful socks?\"\n",
        "print(llm(text))\n"
      ],
      "metadata": {
        "id": "yNCby_VT9qX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "cBZjBk-g9v63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
      ],
      "metadata": {
        "id": "iltZo7oqCtrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "rCoOiRs2Fdt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/google/flan-t5-xl\n",
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0, \"max_length\":40})\n",
        "\n"
      ],
      "metadata": {
        "id": "yU4cLOnHGGmG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Prompt Templates"
      ],
      "metadata": {
        "id": "EQVXXeg9GMxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"Can Barack Obama have a conversation with George Washington?\")"
      ],
      "metadata": {
        "id": "Vf2SpoqXGQnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Question: Can Barack Obama have a conversation with George Washington?\n",
        "\n",
        "Let's think step by step.\n",
        "\n",
        "Answer: \"\"\"\n",
        "llm(prompt)"
      ],
      "metadata": {
        "id": "8r0iQwQJGZvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Let's think step by step.\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "UwtmYbzyGaQV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(question=\"Can Barack Obama have a conversation with George Washington?\")"
      ],
      "metadata": {
        "id": "CHQpXM96Gb7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt)"
      ],
      "metadata": {
        "id": "_Pk4lIQaGdcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Chains:\n",
        "Combine LLMs and Prompts in multi-step workflows"
      ],
      "metadata": {
        "id": "8_B_cRh3Gj0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"Can Barack Obama have a conversation with George Washington?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "bjWwGVyMGmRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba2079c-ff40-4e8a-b63a-fb4ef05b96b2"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " No, Barack Obama cannot have a conversation with George Washington because George Washington is deceased.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Agents and Tools"
      ],
      "metadata": {
        "id": "y76S8zZdG4aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
        "\n",
        "When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
        "\n",
        "Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
        "LLM: The language model powering the agent.\n",
        "Agent: The agent to use."
      ],
      "metadata": {
        "id": "vilB_r-fHDve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent"
      ],
      "metadata": {
        "id": "ueqAOyNGG6Kj"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hcfk378aHLJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "OZRtnkrtHAhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"
      ],
      "metadata": {
        "id": "dEm0LoXBHVPL"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "HlTYj2f2HWBr"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"
      ],
      "metadata": {
        "id": "m476g17xHXTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Memory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Add State to Chains and Agents.\n",
        "Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory."
      ],
      "metadata": {
        "id": "1Ntm-YtYHhH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ],
      "metadata": {
        "id": "eeWY-794HuG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Can we talk about AI?\")"
      ],
      "metadata": {
        "id": "f8cxgWMxHxnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I'm interested in Reinforcement Learning.\")"
      ],
      "metadata": {
        "id": "Bu2LUsmmH0je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Document Loaders\n",
        "\n",
        "Combining language models with your own text data is a powerful way to differentiate them. The first step in doing this is to load the data into “documents” - a fancy way of say some pieces of text. This module is aimed at making this easy."
      ],
      "metadata": {
        "id": "qJzqGWytH2Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import NotionDirectoryLoader\n",
        "\n",
        "loader = NotionDirectoryLoader(\"Notion_DB\")\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "EmZN2YjdH7AG"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Indexes\n",
        "\n",
        "Indexes refer to ways to structure documents so that LLMs can best interact with them. This module contains utility functions for working with documents\n",
        "\n",
        "Embeddings: An embedding is a numerical representation of a piece of information, for example, text, documents, images, audio, etc.\n",
        "Text Splitters: When you want to deal with long pieces of text, it is necessary to split up that text into chunks.\n",
        "Vectorstores: Vector databases store and index vector embeddings from NLP models to understand the meaning and context of strings of text, sentences, and whole documents for more accurate and relevant search results."
      ],
      "metadata": {
        "id": "jO_zrMMuH-Rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
        "  f.write(res.text)"
      ],
      "metadata": {
        "id": "KHDRypG1IFDp"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "T5e3a8OOIG0J"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "EE9UiyQIIIPR"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "JZn5aY5eIKiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "#text = \"This is a test document.\"\n",
        "#query_result = embeddings.embed_query(text)\n",
        "#doc_result = embeddings.embed_documents([text])"
      ],
      "metadata": {
        "id": "3hdFzAVQIMGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "TuuST4wTIOZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = db.similarity_search(query)"
      ],
      "metadata": {
        "id": "Wd5vjlvgIO4Q"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "pn5WomPVIQYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.save_local(\"faiss_index\")\n",
        "new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
        "docs = new_db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "UkB8AWC2IRl2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
